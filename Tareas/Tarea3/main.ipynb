{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 3 - Inteligencia artificial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'/home/ubuntu/inteligencia_artificial/Tareas/Tarea 3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 1\n",
    "Importo dataset (10 puntos)\n",
    "Fuente: https://www.kaggle.com/datasets/ayessa/salary-prediction-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age          workclass  fnlwgt   education  education-num  \\\n",
      "0   39          State-gov   77516   Bachelors             13   \n",
      "1   50   Self-emp-not-inc   83311   Bachelors             13   \n",
      "2   38            Private  215646     HS-grad              9   \n",
      "3   53            Private  234721        11th              7   \n",
      "4   28            Private  338409   Bachelors             13   \n",
      "\n",
      "        marital-status          occupation    relationship    race      sex  \\\n",
      "0        Never-married        Adm-clerical   Not-in-family   White     Male   \n",
      "1   Married-civ-spouse     Exec-managerial         Husband   White     Male   \n",
      "2             Divorced   Handlers-cleaners   Not-in-family   White     Male   \n",
      "3   Married-civ-spouse   Handlers-cleaners         Husband   Black     Male   \n",
      "4   Married-civ-spouse      Prof-specialty            Wife   Black   Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week  native-country  salary  \n",
      "0          2174             0              40   United-States   <=50K  \n",
      "1             0             0              13   United-States   <=50K  \n",
      "2             0             0              40   United-States   <=50K  \n",
      "3             0             0              40   United-States   <=50K  \n",
      "4             0             0              40            Cuba   <=50K  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(21999, 15)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "dataset = pd.read_csv(\"./dataset/train22k.csv\")\n",
    "print(dataset.head())\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 2\n",
    "Entrene el algoritmo KNN sobre los datos seleccionados y aplique predicciones con algún (sub)conjunto DISJUNTO de prueba. Adicionalmente, utilice 3 métricas (a su elección) de error y compárelas entre sí. (15 ptos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7954932777882977\n",
      "hamming_loss: 0.20450672221170232\n",
      "Average precision score:  0.3707464255958146\n",
      "Confusion matrix:\n",
      " [[7846  115]\n",
      " [2045  556]]\n",
      "Porcentajes en confusion matrix: \n",
      " [[74  1]\n",
      " [19  5]]\n",
      "y train  <=50k:  16759  corresponde al:  76.18073548797673 %\n",
      "y train   >50k:  5240  corresponde al:  23.819264512023274 %\n",
      "y validation  <=50k:  7961  corresponde al:  75.37398220034085 %\n",
      "y validation   >50k:  2601  corresponde al:  24.626017799659156 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\"\"\" el siguiente es para asociar strings a numeros \"\"\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "#print(type(dataset.iloc[0,1]))\n",
    "#print(isinstance(dataset.iloc[0,1], str))\n",
    "#print(dataset.head())\n",
    "\"\"\" Declaro el encoder de string a numeros \"\"\"\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "def column_string_to_int(columnas, dataset_validation):\n",
    "     \"\"\" For para iterar el dataset en columnas\"\"\"\n",
    "     for i in range(columnas):\n",
    "          \"\"\"  \n",
    "          En realidad el dataset es un dataframe de pandas, este tiene propiedades para\n",
    "          acceder a él. 'iloc' sirve para acceder a él como una matriz con indices\n",
    "          de fila y columna [x,y].\n",
    "\n",
    "          Con isisntance() puedo ver si la primera fila de datos en cada columna es un string,\n",
    "          ya que en estos casos debo aplicar la conversion a números \"\"\"\n",
    "          if isinstance(dataset_validation.iloc[0,i], str):\n",
    "               \"\"\"\n",
    "               Con dataset.columns[i] obtengo el nombre de la columna,\n",
    "               luego convierto esa columna completa en números \"\"\"\n",
    "               string = dataset_validation.columns[i]\n",
    "               dataset_validation[string] = encoder.fit_transform(dataset_validation[string])\n",
    "\n",
    "def best_k(x_train, y_train, x_test, y_test, max_k):\n",
    "     best_k = 0\n",
    "     best_score = 0\n",
    "     for i in range(max_k):\n",
    "          i = i+1\n",
    "          knn = KNeighborsClassifier(n_neighbors=i)\n",
    "          knn.fit(x_train, y_train)\n",
    "          score = knn.score(x_test, y_test)\n",
    "          if best_score < score:\n",
    "               best_score = score\n",
    "               best_k = i\n",
    "          print(\"Procesando actualmente K=\",i,\", mejor k=\",best_k, \", mayor precision: \", best_score)\n",
    "\n",
    "#Considerar que se debe aplicar tanto para el train como el test\n",
    "def eliminar_columna(x, columna):\n",
    "     x = np.delete(x, columna, 1)\n",
    "     return x\n",
    "\n",
    "def contar_y(y, nombre_dataset=\"\"):\n",
    "     #si es 0 es <=50k, es decir, menor que 50k\n",
    "     counter_menor = 0\n",
    "     counter_mayor = 0\n",
    "     for i in range(len(y)):\n",
    "          if y[i] == 0:\n",
    "               counter_menor += 1\n",
    "          else:\n",
    "               counter_mayor += 1\n",
    "     print(\"y\", nombre_dataset,\" <=50k: \", counter_menor, \" corresponde al: \", counter_menor*100 / len(y), \"%\" )\n",
    "     print(\"y\", nombre_dataset,\"  >50k: \", counter_mayor, \" corresponde al: \", counter_mayor*100 / len(y), \"%\")\n",
    "\n",
    "column_string_to_int(15, dataset)\n",
    "\n",
    "x = dataset.iloc[:, :14].values\n",
    "\n",
    "y = dataset['salary'].values\n",
    "\n",
    "\n",
    "#Para eliminar una columna: \n",
    "\"\"\" print(\"x:\", x)\n",
    "print(\"Cantidad de columnas: \", len(x[0]))\n",
    "x = np.delete(x,7,1)\n",
    "print(\"Cantidad de columnas: \", len(x[0])) \"\"\"\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=17)\n",
    "\n",
    "#x = eliminar_columna(x, 2)\n",
    "\n",
    "#print(x)\n",
    "knn.fit(x, y)\n",
    "#Realizo una predicción\n",
    "\n",
    "dataset_validation = pd.read_csv(\"./dataset/validation.csv\")\n",
    "column_string_to_int(15, dataset_validation)\n",
    "\n",
    "x_validation = dataset_validation.iloc[:, :14].values\n",
    "y_validation = dataset_validation['salary'].values\n",
    "\n",
    "#x_validation = eliminar_columna(x_validation, 2)\n",
    "\n",
    "#knn.predict(x_validation[1].reshape(1,-1))[0]\n",
    "#best_k(x,y,x_validation,y_validation,4)\n",
    "\n",
    "#Predice los resultados \"y\" respecto a la matriz \"x\" de validación (x_validation)\n",
    "y_pred = knn.predict(x_validation)\n",
    "\n",
    "score_train = knn.score(x,y)\n",
    "score_validation = knn.score(x_validation,y_validation)\n",
    "\"\"\" print(\"Score_train: \", score_train)\n",
    "print(\"score_validation: \", score_validation) \"\"\"\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print(\"accuracy_score:\", metrics.accuracy_score(y_validation, y_pred))\n",
    "print(\"hamming_loss:\", metrics.hamming_loss(y_validation, y_pred))\n",
    "print(\"Average precision score: \", metrics.average_precision_score(y_validation, y_pred))\n",
    "print(\"Confusion matrix:\\n\", metrics.confusion_matrix(y_validation, y_pred))\n",
    "\n",
    "matrix_porcentaje = metrics.confusion_matrix(y_validation, y_pred)\n",
    "\n",
    "for i in range(len(matrix_porcentaje)):\n",
    "     for j in range(len(matrix_porcentaje[i])):\n",
    "          matrix_porcentaje[i][j] = matrix_porcentaje[i][j]*100/ len(y_validation)\n",
    "          \n",
    "print(\"Porcentajes en confusion matrix: \\n\", matrix_porcentaje)\n",
    "\n",
    "\n",
    "\n",
    "contar_y(y, \"train\")\n",
    "contar_y(y_validation, \"validation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis\n",
    "\n",
    "Se seleccionaron las siguientes tres  metricas de error:\n",
    "1. **Accuracy score:** Corresponde a la probabilidad de predicciones correctas del modelo ya entrenado. Para ello le damos como parámetros una matriz $x$ que corresponde a los datos en los que se basa el modelo (Son nuevos datos distintos a los de entrenamiento, aunque también se puede ver con los datos de entrenamiento) para generar predicciones y un parámetro $y$ que corresponde a los resultados correctos. Así el output de esta función es:\n",
    "\n",
    "     (total_de_resultados_correctamente_acertados) / (total_de_resultados_correctos)\n",
    "\n",
    "  Se aprecia que las predicciones obtenidas tienen un acierto del 79% aproximadamente, lo cual si bien tiene una tendencia a dar una respuesta correcta, sigue con un porcentaje de error considerable entorno al 21%. Por lo cual no sería una predicción tan confiable, aunque esto es relativo al contexto en el que se necesite.\n",
    "\n",
    "2. **Average precision score:** Corresponde a las nociones que tiene de precisión que se pueden aplicar a cada etiqueta de forma independiente , lo cual entrega una aproximacion del resultado obtenido por y_pred.\n",
    "3. **Matriz de confusión:** La matriz de confusión del inglés Confussion matrix, es una matriz que entrega una forma de visualización de fácil interpretación. Esto se puede explicar de forma más fácil con un diagrama:\n",
    "\n",
    "<img src=\"./img/matriz_confusion.jpg\" alt=\"Matriz de confusion\" width=\"75%\" style=\"\n",
    "  display: block; \n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\" />\n",
    "\n",
    "En donde los verdaderos positivos son los que tienen tanto en la predicción, como en la respuesta real un valor de 0 (<=50k). Los verdaderos negativos son los que en ambos tienen un valor de 1 (>50k). Los falsos positivos son los que en la predicción tienen un valor de 0 (<=50k), pero en la respuesta real un valor de 1 (>50k), por lo que son predicciones erróneas. Los falsos negativos son los que en la predicción son 1 (>50k), pero en el real 0 (<=50k).\n",
    "\n",
    "Al ver la impresón de la confusion matrix con los datos en porcentajes, es aún más fácil realizar una interpretación. Asi se puede observar que los verdaderos positivos (<=50k) son el 74% siendo por lejos la mayor cantidad de resultados. Seguido por un tambien relevante 19% de falsos positivos (<=50k). Teniendo entre estos dos el 94%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 3\n",
    "Entrene el algoritmo de Regresión Lineal ahora con los mismos datos seleccionados, aplicando también predicciones sobre el mismo (sub)conjunto DISJUNTO de prueba de la parte anterior. Elija alguna métrica de error de entre las tres de la parte anterior y compare los errores, dando una explicación de qué sucede. (15 ptos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score_train:  0.25645056075503647\n",
      "score_validation:  0.2729230256309212\n",
      "1.2753269315778546\n"
     ]
    }
   ],
   "source": [
    "# Imports necesarios\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "plt.rcParams['figure.figsize'] = (16, 9)\n",
    "plt.style.use('ggplot')\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "#cargamos los datos de entrada\n",
    "data = pd.read_csv(\"./dataset/train22k.csv\")\n",
    "#veamos cuantas dimensiones y registros contiene\n",
    "data\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "\n",
    "column_string_to_int(15, data)\n",
    "\n",
    "# Asignamos nuestra variable de entrada X para entrenamiento y las etiquetas Y.\n",
    "\n",
    "X_train = data.iloc[:, :14].values\n",
    "#Nocumple = db[db.salary.str.contains('<=50K')]\n",
    "\n",
    "y_train = data['salary'].values\n",
    "\n",
    "# Creamos el objeto de Regresión Linear\n",
    "regr = linear_model.LinearRegression()\n",
    " \n",
    "# Entrenamos nuestro modelo\n",
    "regr.fit(X_train, y_train)\n",
    "y_pred = regr.predict(X_train)\n",
    "#llamamos otro dataset...\n",
    "data_validation = pd.read_csv(\"./dataset/validation.csv\")\n",
    "\n",
    "column_string_to_int(15, data_validation)\n",
    "\n",
    "#print(dataset_validation.head())\n",
    "X_train_validation = data_validation.iloc[:, :14].values\n",
    "y_train_validation = data_validation['salary'].values\n",
    "y_pred1 = regr.predict(X_train_validation)\n",
    "\"\"\" Los datos de salario <=50k son 0 y los >50 son 1 \"\"\"\n",
    "\n",
    "#metricas score...\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "score_train1 = metrics.r2_score(y_train, y_pred)\n",
    "score_validation1 = metrics.r2_score(y_train_validation, y_pred1)\n",
    "\n",
    "print(\"Score_train: \", score_train1)\n",
    "print(\"score_validation: \", score_validation1)\n",
    "\n",
    "print(metrics.max_error(y_train_validation, y_pred1))\n",
    "#listoco...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item 4\n",
    "Tome su mismo dataset y entrene dos algoritmos de clasificación: Gaussian Mixture Model y DBSCAN. Evalúe estos algoritmos sobre el mismo (sub)conjunto DISJUNTO de las partes 2 y 3. Analice la diferencia en los resultados entre los algoritmos (10 ptos). Asimismo, indique y aporte evidencia en la diferencia fundamental (clustering vs. estimación de densidad) que existe entre ambos algoritmos (10 ptos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 3 ... 3 3 1]\n",
      "[3 0 3 ... 3 3 0]\n",
      "-28.94735451565134\n",
      "[0.01609181 0.07073507 0.04250194 0.87067118]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "gm = GaussianMixture(n_components=4, random_state=123).fit(x)\n",
    "#gm.means_\n",
    "#c = gm.predict(x)\n",
    "\n",
    "print(gm.predict(x))\n",
    "print(gm.predict(x_validation))\n",
    "print(gm.score(x))\n",
    "print(gm.weights_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "cluster = DBSCAN(eps=2 , min_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21999, 15)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    39,      7,  77516, ...,   2174,      0,     40],\n",
       "       [    50,      6,  83311, ...,      0,      0,     13],\n",
       "       [    38,      4, 215646, ...,      0,      0,     40],\n",
       "       ...,\n",
       "       [    19,      4, 272063, ...,      0,      0,     35],\n",
       "       [    34,      4, 169564, ...,      0,      0,     40],\n",
       "       [    56,      4, 188856, ...,   8614,      0,     55]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[:, [0,1,2,3,4,5,6,7,8,9,10,11,12]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
